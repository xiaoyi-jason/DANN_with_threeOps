{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220901_bgerr3.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math, time, copy\n",
    "\n",
    "import utils, parameters, Unet_models\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "dir_name        = \"20220901_bgerr\" + str(parameters.background_err)\n",
    "print(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_npz = np.load('dataset/N' + str(parameters.sigNoise) + '_training_set.npz')\n",
    "x_train_obs = training_set_npz['x_train_obs']\n",
    "x_train = training_set_npz['x_train']\n",
    "mask_train = training_set_npz['mask_train']\n",
    "\n",
    "x_val_obs = training_set_npz['x_val_obs']\n",
    "x_val = training_set_npz['x_val']\n",
    "mask_val = training_set_npz['mask_val']\n",
    "\n",
    "stdTr = training_set_npz['std']\n",
    "meanTr = training_set_npz['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_dataset  = torch.utils.data.TensorDataset(torch.Tensor(x_train_obs), torch.Tensor(x_train), torch.Tensor(mask_train))\n",
    "val_dataset       = torch.utils.data.TensorDataset(torch.Tensor(x_val_obs),  torch.Tensor(x_val), torch.Tensor(mask_val)) \n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True),\n",
    "}\n",
    "\n",
    "dataset_sizes = {'train': len(training_dataset), 'val': len(val_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head = Unet_models.L96_UnetConvRec_head().to(device)\n",
    "model_head.load_state_dict(torch.load(\"ckpts/\" + dir_name + \"/pretrain_head_epoch20\"))\n",
    "\n",
    "model_dyn = Unet_models.L96_UnetConvRec_dyn().to(device)\n",
    "\n",
    "optimizer_model_dyn = optim.Adam(model_dyn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python/virtualenv/py3.6-gpu/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyn loss(gt): 2.9160e-02\n",
      "train rec loss: 7.9870e-01 dyn loss: 4.1299e-02 dyn loss(bg): 5.1809e-02 loss_R: 6.2948e-01 loss_I: 8.2287e-01\n",
      "dyn loss(gt): 2.9154e-02\n",
      "val rec loss: 8.7033e-01 dyn loss: 2.7097e-02 dyn loss(bg): 3.6897e-02 loss_R: 6.5272e-01 loss_I: 9.0142e-01\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train rec loss: 8.1036e-01 dyn loss: 2.6508e-02 dyn loss(bg): 3.4007e-02 loss_R: 6.3912e-01 loss_I: 8.3482e-01\n",
      "val rec loss: 9.5383e-01 dyn loss: 2.7035e-02 dyn loss(bg): 3.1760e-02 loss_R: 7.3533e-01 loss_I: 9.8505e-01\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train rec loss: 8.8692e-01 dyn loss: 2.5217e-02 dyn loss(bg): 2.8085e-02 loss_R: 7.1780e-01 loss_I: 9.1108e-01\n",
      "val rec loss: 1.0300e+00 dyn loss: 3.1821e-02 dyn loss(bg): 2.7034e-02 loss_R: 8.1663e-01 loss_I: 1.0605e+00\n",
      "Training complete in 1m 38s\n",
      "Best val reconstruction loss: 8.703303e-01\n"
     ]
    }
   ],
   "source": [
    "# training function for Generator\n",
    "since = time.time()\n",
    "\n",
    "best_model_dyn_wts = copy.deepcopy(model_dyn.state_dict())\n",
    "\n",
    "best_loss_rec = 1e10\n",
    "\n",
    "train_loss_rec_list = []\n",
    "val_loss_rec_list = []\n",
    "train_loss_dyn_list = []\n",
    "val_loss_dyn_list = []\n",
    "train_loss_dynbg_list = []\n",
    "val_loss_dynbg_list = []\n",
    "train_loss_R_list = []\n",
    "val_loss_R_list = []\n",
    "train_loss_I_list = []\n",
    "val_loss_I_list = []\n",
    "\n",
    "num_epochs = 200\n",
    "model_head.eval()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model_dyn.train()\n",
    "        else:\n",
    "            model_dyn.eval()\n",
    "\n",
    "        running_loss_rec    = 0.0\n",
    "        running_loss_dyn    = 0.0\n",
    "        running_loss_dyn_bg = 0.0\n",
    "        running_loss_dyn_gt = 0.0\n",
    "        running_loss_R      = 0.0\n",
    "        running_loss_I      = 0.0\n",
    "        num_loss            = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, targets, mask, in dataloaders[phase]:\n",
    "            mask        = mask.to(device)\n",
    "            targets     = targets.to(device)\n",
    "            inputs      = inputs.to(device)\n",
    "            \n",
    "            optimizer_model_dyn.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True): \n",
    "                inputs     = model_head(inputs * mask)\n",
    "                outputs    = model_dyn(inputs)\n",
    "                \n",
    "                loss_rec    = torch.mean((outputs - targets)**2)\n",
    "                loss_dyn_bg = utils.dynamic_loss(outputs, 1, meanTr, stdTr, 3)\n",
    "                loss_dyn_gt = utils.dynamic_loss(targets, 1, meanTr, stdTr, 3)\n",
    "                loss_dyn    = utils.dynamic_loss(outputs, 1, meanTr, stdTr, 1)\n",
    "                loss_R      = torch.sum((outputs - targets)**2 * mask) / torch.sum(mask)\n",
    "                loss_I      = torch.sum((outputs - targets)**2 * (1 - mask)) / torch.sum(1 - mask)\n",
    "\n",
    "                loss       = loss_dyn_bg\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer_model_dyn.step()\n",
    "\n",
    "            running_loss_rec         += loss_rec.item()    * inputs.size(0) * stdTr**2\n",
    "            running_loss_dyn         += loss_dyn.item()    * inputs.size(0) * stdTr**2\n",
    "            running_loss_dyn_bg      += loss_dyn_bg.item() * inputs.size(0) * stdTr**2\n",
    "            running_loss_dyn_gt      += loss_dyn_gt.item() * inputs.size(0) * stdTr**2\n",
    "            running_loss_R           += loss_R.item()      * inputs.size(0) * stdTr**2\n",
    "            running_loss_I           += loss_I.item()      * inputs.size(0) * stdTr**2\n",
    "            num_loss                 += inputs.size(0)\n",
    "\n",
    "        epoch_loss_rec       = running_loss_rec    / num_loss\n",
    "        epoch_loss_dyn       = running_loss_dyn    / num_loss\n",
    "        epoch_loss_dyn_bg    = running_loss_dyn_bg / num_loss\n",
    "        epoch_loss_dyn_gt    = running_loss_dyn_gt / num_loss\n",
    "        epoch_loss_R         = running_loss_R      / num_loss\n",
    "        epoch_loss_I         = running_loss_I      / num_loss\n",
    "        \n",
    "        if epoch == 0:\n",
    "            print('dyn loss(gt): {:.4e}'.format(epoch_loss_dyn_gt))\n",
    "        print('{} rec loss: {:.4e} dyn loss: {:.4e} dyn loss(bg): {:.4e} loss_R: {:.4e} loss_I: {:.4e}'.format(\n",
    "            phase, epoch_loss_rec, epoch_loss_dyn, epoch_loss_dyn_bg, epoch_loss_R, epoch_loss_I))\n",
    "        \n",
    "        if phase == 'train':\n",
    "            train_loss_rec_list.append(epoch_loss_rec)\n",
    "            train_loss_dyn_list.append(epoch_loss_dyn)\n",
    "            train_loss_dynbg_list.append(epoch_loss_dyn_bg)\n",
    "            train_loss_R_list.append(epoch_loss_R)\n",
    "            train_loss_I_list.append(epoch_loss_I)\n",
    "        else:\n",
    "            val_loss_rec_list.append(epoch_loss_rec)\n",
    "            val_loss_dyn_list.append(epoch_loss_dyn)\n",
    "            val_loss_dynbg_list.append(epoch_loss_dyn_bg)\n",
    "            val_loss_R_list.append(epoch_loss_R)\n",
    "            val_loss_I_list.append(epoch_loss_I)\n",
    "\n",
    "        if phase == 'val' and epoch_loss_rec < best_loss_rec:\n",
    "            best_loss_rec = epoch_loss_rec\n",
    "            best_model_dyn_wts = copy.deepcopy(model_dyn.state_dict())\n",
    "\n",
    "    if epoch_loss_dyn_bg < parameters.relative_err:\n",
    "        break\n",
    "        \n",
    "    print()\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val reconstruction loss: {:4e}'.format(best_loss_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model at ckpts/20220901_bgerr3.5/pretrain_dyn_epoch3\n"
     ]
    }
   ],
   "source": [
    "save_dir_model_dyn = \"ckpts/\" + dir_name + \"/pretrain_dyn_epoch\" + str(epoch + 1)\n",
    "print(\"saving model at \" + save_dir_model_dyn)\n",
    "torch.save(model_dyn.state_dict(), save_dir_model_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving loss at train_loss/20220901_bgerr3.5/pretrain_dyn_epoch3\n"
     ]
    }
   ],
   "source": [
    "save_dir_loss_dyn  = \"train_loss/\" + dir_name + \"/pretrain_dyn_epoch\" + str(epoch + 1)\n",
    "print(\"saving loss at \" + save_dir_loss_dyn)\n",
    "np.savez(save_dir_loss_dyn,\n",
    "         train_loss_rec   = train_loss_rec_list,   val_loss_rec   = val_loss_rec_list, \n",
    "         train_loss_dyn   = train_loss_dyn_list,   val_loss_dyn   = val_loss_dyn_list,\n",
    "         train_loss_dynbg = train_loss_dynbg_list, val_loss_dynbg = val_loss_dynbg_list,\n",
    "         train_loss_R     = train_loss_R_list,     val_loss_R     = val_loss_R_list, \n",
    "         train_loss_I     = train_loss_I_list,     val_loss_I     = val_loss_I_list,\n",
    "         time = time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
