{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220829_bgerr4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math, time, copy\n",
    "\n",
    "import utils, parameters, Unet_models\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "dir_name        = \"20220829_bgerr\" + str(parameters.background_err)\n",
    "print(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_npz = np.load('dataset/N' + str(parameters.sigNoise) + '_training_set.npz')\n",
    "x_train_obs = training_set_npz['x_train_obs']\n",
    "x_train = training_set_npz['x_train']\n",
    "mask_train = training_set_npz['mask_train']\n",
    "\n",
    "x_val_obs = training_set_npz['x_val_obs']\n",
    "x_val = training_set_npz['x_val']\n",
    "mask_val = training_set_npz['mask_val']\n",
    "\n",
    "stdTr = training_set_npz['std']\n",
    "meanTr = training_set_npz['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_dataset  = torch.utils.data.TensorDataset(torch.Tensor(x_train_obs), torch.Tensor(x_train), torch.Tensor(mask_train))\n",
    "val_dataset       = torch.utils.data.TensorDataset(torch.Tensor(x_val_obs),  torch.Tensor(x_val), torch.Tensor(mask_val)) \n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True),\n",
    "}\n",
    "\n",
    "dataset_sizes = {'train': len(training_dataset), 'val': len(val_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head  = Unet_models.L63_UnetConvRec_head().to(device)\n",
    "model_dyn   = Unet_models.L63_UnetConvRec_dyn().to(device)\n",
    "model_sup   = Unet_models.L63_UnetConvRec_sup().to(device)\n",
    "\n",
    "model_head.load_state_dict(torch.load(\"ckpts/\" + dir_name + \"/pretrain_head_epoch20\"))\n",
    "model_dyn.load_state_dict(torch.load(\"ckpts/\" + dir_name + \"/pretrain_dyn_epoch2\"))\n",
    "\n",
    "best_model_head_wts  = copy.deepcopy(model_head.state_dict())\n",
    "best_model_dyn_wts   = copy.deepcopy(model_dyn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1\n",
      "finetuning model_sup...\n",
      "optimizing model_sup, lr =  0.0001\n",
      "optimizing model_head, lr =  0.0001\n",
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python/virtualenv/py3.6-gpu/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rec loss: 1.7033e+01 dyn loss: 1.2604e-01 dyn loss(bg): 1.2301e-01 loss_R: 8.2304e+00 loss_I: 1.7368e+01\n",
      "val rec loss: 1.9518e+00 dyn loss: 7.5354e-02 dyn loss(bg): 7.8525e-02 loss_R: 5.0198e-01 loss_I: 2.0070e+00\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train rec loss: 1.6941e+00 dyn loss: 6.1024e-02 dyn loss(bg): 6.5899e-02 loss_R: 4.2372e-01 loss_I: 1.7425e+00\n",
      "val rec loss: 1.6465e+00 dyn loss: 5.1069e-02 dyn loss(bg): 5.7934e-02 loss_R: 4.2282e-01 loss_I: 1.6931e+00\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train rec loss: 1.5141e+00 dyn loss: 4.1057e-02 dyn loss(bg): 4.7236e-02 loss_R: 3.9727e-01 loss_I: 1.5566e+00\n",
      "val rec loss: 1.5210e+00 dyn loss: 3.5374e-02 dyn loss(bg): 4.2562e-02 loss_R: 3.9686e-01 loss_I: 1.5637e+00\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train rec loss: 1.4364e+00 dyn loss: 2.9290e-02 dyn loss(bg): 3.6070e-02 loss_R: 3.9086e-01 loss_I: 1.4762e+00\n",
      "val rec loss: 1.4738e+00 dyn loss: 2.6273e-02 dyn loss(bg): 3.4498e-02 loss_R: 3.8881e-01 loss_I: 1.5151e+00\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train rec loss: 1.3997e+00 dyn loss: 2.2212e-02 dyn loss(bg): 2.9314e-02 loss_R: 3.8663e-01 loss_I: 1.4382e+00\n",
      "val rec loss: 1.4423e+00 dyn loss: 1.9762e-02 dyn loss(bg): 2.8305e-02 loss_R: 3.9296e-01 loss_I: 1.4822e+00\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train rec loss: 1.3740e+00 dyn loss: 1.7672e-02 dyn loss(bg): 2.4921e-02 loss_R: 3.8809e-01 loss_I: 1.4116e+00\n",
      "val rec loss: 1.4555e+00 dyn loss: 1.6974e-02 dyn loss(bg): 2.5148e-02 loss_R: 3.8676e-01 loss_I: 1.4962e+00\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train rec loss: 1.3501e+00 dyn loss: 1.4887e-02 dyn loss(bg): 2.2242e-02 loss_R: 3.8299e-01 loss_I: 1.3869e+00\n",
      "val rec loss: 1.4307e+00 dyn loss: 1.4316e-02 dyn loss(bg): 2.2308e-02 loss_R: 4.0059e-01 loss_I: 1.4699e+00\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train rec loss: 1.3378e+00 dyn loss: 1.3033e-02 dyn loss(bg): 2.0406e-02 loss_R: 3.8338e-01 loss_I: 1.3741e+00\n",
      "val rec loss: 1.4042e+00 dyn loss: 1.2735e-02 dyn loss(bg): 2.0377e-02 loss_R: 3.8658e-01 loss_I: 1.4429e+00\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train rec loss: 1.3137e+00 dyn loss: 1.1662e-02 dyn loss(bg): 1.9051e-02 loss_R: 3.7694e-01 loss_I: 1.3493e+00\n",
      "val rec loss: 1.3831e+00 dyn loss: 1.1640e-02 dyn loss(bg): 2.0359e-02 loss_R: 3.7902e-01 loss_I: 1.4214e+00\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train rec loss: 1.2933e+00 dyn loss: 1.1029e-02 dyn loss(bg): 1.8433e-02 loss_R: 3.7619e-01 loss_I: 1.3282e+00\n",
      "val rec loss: 1.3906e+00 dyn loss: 1.1317e-02 dyn loss(bg): 1.8727e-02 loss_R: 3.9356e-01 loss_I: 1.4286e+00\n",
      "\n",
      "Training complete in 4m 16s\n",
      "Best val reconstruction loss: 1.383135e+00\n",
      "saving model_head at ckpts/20220829_bgerr4.0/finetune2_head_epoch10\n",
      "saving model_sup at ckpts/20220829_bgerr4.0/finetune2_sup_epoch10\n",
      "saving model_dyn at ckpts/20220829_bgerr4.0/finetune2_dyn_epoch10\n",
      "saving loss at train_loss/20220829_bgerr4.0/finetune2_epoch10\n",
      "\n",
      "finetuning model_dyn...\n",
      "optimizing model_dyn, lr =  0.0001\n",
      "Epoch 0/9\n",
      "----------\n",
      "train rec loss: 4.4964e+00 dyn loss: 4.6958e-02 dyn loss(bg): 4.4700e-02 loss_R: 3.0277e+00 loss_I: 4.5523e+00\n",
      "val rec loss: 2.5136e+00 dyn loss: 2.1263e-02 dyn loss(bg): 1.9927e-02 loss_R: 6.2436e-01 loss_I: 2.5855e+00\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train rec loss: 2.1089e+00 dyn loss: 1.4872e-02 dyn loss(bg): 1.3553e-02 loss_R: 5.0501e-01 loss_I: 2.1699e+00\n",
      "val rec loss: 1.9961e+00 dyn loss: 1.1956e-02 dyn loss(bg): 1.1261e-02 loss_R: 5.0536e-01 loss_I: 2.0528e+00\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train rec loss: 1.6649e+00 dyn loss: 9.2073e-03 dyn loss(bg): 8.3465e-03 loss_R: 4.7197e-01 loss_I: 1.7103e+00\n",
      "val rec loss: 1.7177e+00 dyn loss: 8.6875e-03 dyn loss(bg): 7.9648e-03 loss_R: 5.3458e-01 loss_I: 1.7627e+00\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train rec loss: 1.4438e+00 dyn loss: 6.3465e-03 dyn loss(bg): 5.6486e-03 loss_R: 4.7268e-01 loss_I: 1.4807e+00\n",
      "val rec loss: 1.5129e+00 dyn loss: 6.5916e-03 dyn loss(bg): 6.0628e-03 loss_R: 5.0729e-01 loss_I: 1.5511e+00\n",
      "Training complete in 2m 0s\n",
      "Best val reconstruction loss: 1.512861e+00\n",
      "saving model_head at ckpts/20220829_bgerr4.0/finetune3_head_epoch10\n",
      "saving model_sup at ckpts/20220829_bgerr4.0/finetune3_sup_epoch10\n",
      "saving model_dyn at ckpts/20220829_bgerr4.0/finetune3_dyn_epoch10\n",
      "saving loss at train_loss/20220829_bgerr4.0/finetune3_epoch10\n",
      "\n",
      "step:  2\n",
      "finetuning model_sup...\n",
      "optimizing model_sup, lr =  0.0001\n",
      "optimizing model_head, lr =  0\n",
      "Epoch 0/9\n",
      "----------\n",
      "train rec loss: 1.4879e+00 dyn loss: 8.5337e-03 dyn loss(bg): 1.0022e-02 loss_R: 4.7383e-01 loss_I: 1.5265e+00\n",
      "val rec loss: 1.5155e+00 dyn loss: 8.5824e-03 dyn loss(bg): 1.0731e-02 loss_R: 4.4849e-01 loss_I: 1.5561e+00\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train rec loss: 1.3678e+00 dyn loss: 8.7988e-03 dyn loss(bg): 1.1288e-02 loss_R: 4.3009e-01 loss_I: 1.4035e+00\n",
      "val rec loss: 1.4654e+00 dyn loss: 1.0135e-02 dyn loss(bg): 1.3331e-02 loss_R: 4.3802e-01 loss_I: 1.5045e+00\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train rec loss: 1.3358e+00 dyn loss: 9.1927e-03 dyn loss(bg): 1.2085e-02 loss_R: 4.1378e-01 loss_I: 1.3709e+00\n",
      "val rec loss: 1.4459e+00 dyn loss: 1.0059e-02 dyn loss(bg): 1.3350e-02 loss_R: 4.4666e-01 loss_I: 1.4839e+00\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train rec loss: 1.3064e+00 dyn loss: 9.3891e-03 dyn loss(bg): 1.2580e-02 loss_R: 4.0333e-01 loss_I: 1.3408e+00\n",
      "val rec loss: 1.4109e+00 dyn loss: 9.8896e-03 dyn loss(bg): 1.3525e-02 loss_R: 4.1135e-01 loss_I: 1.4490e+00\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train rec loss: 1.2808e+00 dyn loss: 9.1342e-03 dyn loss(bg): 1.2529e-02 loss_R: 3.9516e-01 loss_I: 1.3145e+00\n",
      "val rec loss: 1.4145e+00 dyn loss: 9.6715e-03 dyn loss(bg): 1.3765e-02 loss_R: 4.3867e-01 loss_I: 1.4517e+00\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train rec loss: 1.2655e+00 dyn loss: 9.0287e-03 dyn loss(bg): 1.2680e-02 loss_R: 3.8786e-01 loss_I: 1.2989e+00\n",
      "val rec loss: 1.3603e+00 dyn loss: 9.8042e-03 dyn loss(bg): 1.4184e-02 loss_R: 3.8748e-01 loss_I: 1.3973e+00\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train rec loss: 1.2502e+00 dyn loss: 8.7980e-03 dyn loss(bg): 1.2615e-02 loss_R: 3.8359e-01 loss_I: 1.2832e+00\n",
      "val rec loss: 1.3835e+00 dyn loss: 9.4209e-03 dyn loss(bg): 1.4030e-02 loss_R: 4.0145e-01 loss_I: 1.4209e+00\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train rec loss: 1.2348e+00 dyn loss: 8.6344e-03 dyn loss(bg): 1.2660e-02 loss_R: 3.7975e-01 loss_I: 1.2673e+00\n",
      "val rec loss: 1.3413e+00 dyn loss: 9.1426e-03 dyn loss(bg): 1.3916e-02 loss_R: 3.6477e-01 loss_I: 1.3785e+00\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train rec loss: 1.2270e+00 dyn loss: 8.3737e-03 dyn loss(bg): 1.2480e-02 loss_R: 3.7875e-01 loss_I: 1.2593e+00\n",
      "val rec loss: 1.3457e+00 dyn loss: 9.6489e-03 dyn loss(bg): 1.4816e-02 loss_R: 4.0443e-01 loss_I: 1.3815e+00\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train rec loss: 1.2130e+00 dyn loss: 8.5431e-03 dyn loss(bg): 1.2792e-02 loss_R: 3.7271e-01 loss_I: 1.2450e+00\n",
      "val rec loss: 1.3072e+00 dyn loss: 8.8488e-03 dyn loss(bg): 1.3894e-02 loss_R: 3.7118e-01 loss_I: 1.3429e+00\n",
      "\n",
      "Training complete in 5m 52s\n",
      "Best val reconstruction loss: 1.307222e+00\n",
      "saving model_head at ckpts/20220829_bgerr4.0/finetune4_head_epoch10\n",
      "saving model_sup at ckpts/20220829_bgerr4.0/finetune4_sup_epoch10\n",
      "saving model_dyn at ckpts/20220829_bgerr4.0/finetune4_dyn_epoch10\n",
      "saving loss at train_loss/20220829_bgerr4.0/finetune4_epoch10\n",
      "\n",
      "finetuning model_dyn...\n",
      "optimizing model_dyn, lr =  0.0001\n",
      "Epoch 0/9\n",
      "----------\n",
      "train rec loss: 1.3072e+00 dyn loss: 6.3004e-03 dyn loss(bg): 5.3467e-03 loss_R: 5.0159e-01 loss_I: 1.3379e+00\n",
      "val rec loss: 1.4138e+00 dyn loss: 5.5536e-03 dyn loss(bg): 4.4002e-03 loss_R: 4.5679e-01 loss_I: 1.4503e+00\n",
      "Training complete in 0m 39s\n",
      "Best val reconstruction loss: 1.413836e+00\n",
      "saving model_head at ckpts/20220829_bgerr4.0/finetune5_head_epoch10\n",
      "saving model_sup at ckpts/20220829_bgerr4.0/finetune5_sup_epoch10\n",
      "saving model_dyn at ckpts/20220829_bgerr4.0/finetune5_dyn_epoch10\n",
      "saving loss at train_loss/20220829_bgerr4.0/finetune5_epoch10\n",
      "\n",
      "step:  3\n",
      "finetuning model_sup...\n",
      "optimizing model_sup, lr =  0.0001\n",
      "optimizing model_head, lr =  0\n",
      "Epoch 0/9\n",
      "----------\n",
      "train rec loss: 1.2508e+00 dyn loss: 5.2932e-03 dyn loss(bg): 6.1343e-03 loss_R: 4.3830e-01 loss_I: 1.2818e+00\n",
      "val rec loss: 1.4163e+00 dyn loss: 6.0319e-03 dyn loss(bg): 8.0602e-03 loss_R: 4.6330e-01 loss_I: 1.4526e+00\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train rec loss: 1.2172e+00 dyn loss: 5.5535e-03 dyn loss(bg): 7.5655e-03 loss_R: 4.1654e-01 loss_I: 1.2477e+00\n",
      "val rec loss: 1.3202e+00 dyn loss: 6.5574e-03 dyn loss(bg): 9.3957e-03 loss_R: 4.5749e-01 loss_I: 1.3531e+00\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train rec loss: 1.1886e+00 dyn loss: 5.8505e-03 dyn loss(bg): 8.2966e-03 loss_R: 4.0288e-01 loss_I: 1.2185e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val rec loss: 1.3152e+00 dyn loss: 6.8588e-03 dyn loss(bg): 1.0134e-02 loss_R: 3.9696e-01 loss_I: 1.3501e+00\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train rec loss: 1.1724e+00 dyn loss: 6.2446e-03 dyn loss(bg): 8.9736e-03 loss_R: 3.9751e-01 loss_I: 1.2019e+00\n",
      "val rec loss: 1.3070e+00 dyn loss: 7.3196e-03 dyn loss(bg): 1.0909e-02 loss_R: 3.8662e-01 loss_I: 1.3420e+00\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train rec loss: 1.1736e+00 dyn loss: 6.3382e-03 dyn loss(bg): 9.2162e-03 loss_R: 3.9549e-01 loss_I: 1.2032e+00\n",
      "val rec loss: 1.2810e+00 dyn loss: 6.3396e-03 dyn loss(bg): 9.5218e-03 loss_R: 4.0076e-01 loss_I: 1.3145e+00\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train rec loss: 1.1719e+00 dyn loss: 6.4874e-03 dyn loss(bg): 9.3791e-03 loss_R: 3.9556e-01 loss_I: 1.2014e+00\n",
      "val rec loss: 1.2883e+00 dyn loss: 6.6447e-03 dyn loss(bg): 9.6022e-03 loss_R: 4.1723e-01 loss_I: 1.3215e+00\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train rec loss: 1.1561e+00 dyn loss: 6.8971e-03 dyn loss(bg): 9.9607e-03 loss_R: 3.8622e-01 loss_I: 1.1854e+00\n",
      "val rec loss: 1.3739e+00 dyn loss: 8.0501e-03 dyn loss(bg): 1.1684e-02 loss_R: 5.5615e-01 loss_I: 1.4050e+00\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train rec loss: 1.1479e+00 dyn loss: 6.7519e-03 dyn loss(bg): 9.8829e-03 loss_R: 3.8583e-01 loss_I: 1.1769e+00\n",
      "val rec loss: 1.3346e+00 dyn loss: 6.5833e-03 dyn loss(bg): 1.0001e-02 loss_R: 3.7245e-01 loss_I: 1.3712e+00\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train rec loss: 1.1295e+00 dyn loss: 6.9563e-03 dyn loss(bg): 1.0276e-02 loss_R: 3.7649e-01 loss_I: 1.1582e+00\n",
      "val rec loss: 1.2502e+00 dyn loss: 8.0780e-03 dyn loss(bg): 1.2355e-02 loss_R: 3.8939e-01 loss_I: 1.2830e+00\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train rec loss: 1.1310e+00 dyn loss: 7.0413e-03 dyn loss(bg): 1.0368e-02 loss_R: 3.7739e-01 loss_I: 1.1596e+00\n",
      "val rec loss: 1.2246e+00 dyn loss: 7.3185e-03 dyn loss(bg): 1.1148e-02 loss_R: 3.7653e-01 loss_I: 1.2569e+00\n",
      "\n",
      "Training complete in 7m 32s\n",
      "Best val reconstruction loss: 1.224632e+00\n",
      "saving model_head at ckpts/20220829_bgerr4.0/finetune6_head_epoch10\n",
      "saving model_sup at ckpts/20220829_bgerr4.0/finetune6_sup_epoch10\n",
      "saving model_dyn at ckpts/20220829_bgerr4.0/finetune6_dyn_epoch10\n",
      "saving loss at train_loss/20220829_bgerr4.0/finetune6_epoch10\n",
      "\n",
      "finetuning model_dyn...\n",
      "optimizing model_dyn, lr =  0.0001\n",
      "Epoch 0/9\n",
      "----------\n",
      "train rec loss: 1.2705e+00 dyn loss: 6.1723e-03 dyn loss(bg): 4.9502e-03 loss_R: 5.2420e-01 loss_I: 1.2989e+00\n",
      "val rec loss: 1.3693e+00 dyn loss: 4.7353e-03 dyn loss(bg): 3.6141e-03 loss_R: 6.4375e-01 loss_I: 1.3970e+00\n",
      "Training complete in 0m 48s\n",
      "Best val reconstruction loss: 1.369341e+00\n",
      "saving model_head at ckpts/20220829_bgerr4.0/finetune7_head_epoch10\n",
      "saving model_sup at ckpts/20220829_bgerr4.0/finetune7_sup_epoch10\n",
      "saving model_dyn at ckpts/20220829_bgerr4.0/finetune7_dyn_epoch10\n",
      "saving loss at train_loss/20220829_bgerr4.0/finetune7_epoch10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs_sup  = [   10,   10,   10]\n",
    "num_epochs_dyn  = [   10,   10,   10]\n",
    "lr_head         = [ 1e-4,    0,    0]\n",
    "# lr_head         = [    0,    0,    0]\n",
    "lr_sup          = [ 1e-4, 1e-4, 1e-4]\n",
    "lr_dyn          = [ 1e-4, 1e-4, 1e-4]\n",
    "num_epochs_list = [num_epochs_sup, num_epochs_dyn]\n",
    "mod_name        = [\"sup\", \"dyn\"]\n",
    "\n",
    "if not os.path.exists(\"ckpts/\" + dir_name):\n",
    "    os.makedirs(\"ckpts/\" + dir_name)\n",
    "    print(\"creating dir: ckpts/\" + dir_name )\n",
    "    \n",
    "if not os.path.exists(\"train_loss/\" + dir_name):\n",
    "    os.makedirs(\"train_loss/\" + dir_name)\n",
    "    print(\"creating dir: train_loss/\" + dir_name )\n",
    "\n",
    "for step in range(1, 4):\n",
    "\n",
    "    print(\"step: \", step)\n",
    "    model = Unet_models.L63_DARNN(model_head, model_dyn, model_sup, step)\n",
    "    \n",
    "    for status in range(2): # status=0 -> train sup, status=1 -> train dyn\n",
    "        \n",
    "        if status == 0:\n",
    "            print(\"finetuning model_sup...\")\n",
    "        else:\n",
    "            print(\"finetuning model_dyn...\")\n",
    "\n",
    "        since = time.time()\n",
    "        best_loss_rec = 1e10\n",
    "\n",
    "        train_loss_rec_list = []\n",
    "        val_loss_rec_list = []\n",
    "        train_loss_dyn_list = []\n",
    "        val_loss_dyn_list = []\n",
    "        train_loss_dynbg_list = []\n",
    "        val_loss_dynbg_list = []\n",
    "        train_loss_R_list = []\n",
    "        val_loss_R_list = []\n",
    "        train_loss_I_list = []\n",
    "        val_loss_I_list = []\n",
    "        \n",
    "        num_epochs = num_epochs_list[status][step-1]\n",
    "        \n",
    "        if status == 0:\n",
    "            model.model_dyn.eval()\n",
    "            optimizer_model_sup  = optim.Adam(model.model_sup.parameters(),  lr=lr_sup[step-1])\n",
    "            optimizer_model_head = optim.Adam(model.model_head.parameters(), lr=lr_head[step-1])\n",
    "            print(\"optimizing model_sup, lr = \",  lr_sup[step-1])\n",
    "            print(\"optimizing model_head, lr = \", lr_head[step-1])\n",
    "            \n",
    "        else:\n",
    "            model.model_sup.eval()\n",
    "            model.model_head.eval()\n",
    "            optimizer_model_dyn  = optim.Adam(model.model_dyn.parameters(),  lr=lr_dyn[step-1])\n",
    "            print(\"optimizing model_dyn, lr = \",  lr_dyn[step-1])\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    if status == 0:\n",
    "                        model.model_sup.train()\n",
    "                        model.model_head.train()\n",
    "                    else:\n",
    "                        model.model_dyn.train()\n",
    "                else:\n",
    "                    if status == 0:\n",
    "                        model.model_sup.eval()\n",
    "                        model.model_head.eval()\n",
    "                    else:\n",
    "                        model.model_dyn.eval()\n",
    "\n",
    "                running_loss_rec    = 0.0\n",
    "                running_loss_dyn    = 0.0\n",
    "                running_loss_dyn_bg = 0.0\n",
    "                running_loss_R      = 0.0\n",
    "                running_loss_I      = 0.0\n",
    "                num_loss            = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, targets, mask, in dataloaders[phase]:\n",
    "                    mask        = mask.to(device)\n",
    "                    targets     = targets.to(device)\n",
    "                    inputs      = inputs.to(device)\n",
    "                    \n",
    "                    if status == 0:\n",
    "                        optimizer_model_sup.zero_grad()\n",
    "                        optimizer_model_head.zero_grad()\n",
    "                    else:\n",
    "                        optimizer_model_dyn.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(True): \n",
    "                        outputs    = model(inputs[:,0:1,:], mask[:,0:1,:])\n",
    "                        \n",
    "                        dyn_list   = torch.zeros(step + 1)\n",
    "                        dynbg_list = torch.zeros(step + 1)\n",
    "                        for ii, elem in enumerate(outputs):\n",
    "                            dyn_list[ii]   = utils.dynamic_loss(elem, 1, meanTr, stdTr, 1)\n",
    "                            dynbg_list[ii] = utils.dynamic_loss(elem, 1, meanTr, stdTr, 3)\n",
    "                            \n",
    "                        output      = outputs[-1]\n",
    "                        loss_rec    = torch.mean((output - targets)**2)\n",
    "                        loss_dyn    = dyn_list[-1]\n",
    "                        loss_dyn_bg = dynbg_list[-1]\n",
    "                        loss_R      = torch.sum((output - targets)**2 * mask) / torch.sum(mask)\n",
    "                        loss_I      = torch.sum((output - targets)**2 * (1 - mask)) / torch.sum(1 - mask)\n",
    "                        \n",
    "                        if status == 0:\n",
    "                            loss  = loss_rec\n",
    "                        else:\n",
    "                            loss  = loss_rec + 100 * torch.mean(dynbg_list)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            if status == 0:\n",
    "                                optimizer_model_sup.step()\n",
    "                                optimizer_model_head.step()\n",
    "                            else:\n",
    "                                optimizer_model_dyn.step()\n",
    "\n",
    "                    running_loss_rec         += loss_rec.item()    * inputs.size(0) * stdTr**2\n",
    "                    running_loss_dyn         += loss_dyn.item()    * inputs.size(0) * stdTr**2\n",
    "                    running_loss_dyn_bg      += loss_dyn_bg.item() * inputs.size(0) * stdTr**2\n",
    "                    running_loss_R           += loss_R.item()      * inputs.size(0) * stdTr**2\n",
    "                    running_loss_I           += loss_I.item()      * inputs.size(0) * stdTr**2\n",
    "                    num_loss                 += inputs.size(0)\n",
    "\n",
    "                epoch_loss_rec       = running_loss_rec    / num_loss\n",
    "                epoch_loss_dyn       = running_loss_dyn    / num_loss\n",
    "                epoch_loss_dyn_bg    = running_loss_dyn_bg / num_loss\n",
    "                epoch_loss_R         = running_loss_R      / num_loss\n",
    "                epoch_loss_I         = running_loss_I      / num_loss\n",
    "\n",
    "                print('{} rec loss: {:.4e} dyn loss: {:.4e} dyn loss(bg): {:.4e} loss_R: {:.4e} loss_I: {:.4e}'.format(\n",
    "                    phase, epoch_loss_rec, epoch_loss_dyn, epoch_loss_dyn_bg, epoch_loss_R, epoch_loss_I))\n",
    "\n",
    "                if phase == 'train':\n",
    "                    train_loss_rec_list.append(epoch_loss_rec)\n",
    "                    train_loss_dyn_list.append(epoch_loss_dyn)\n",
    "                    train_loss_dynbg_list.append(epoch_loss_dyn_bg)\n",
    "                    train_loss_R_list.append(epoch_loss_R)\n",
    "                    train_loss_I_list.append(epoch_loss_I)\n",
    "                else:\n",
    "                    val_loss_rec_list.append(epoch_loss_rec)\n",
    "                    val_loss_dyn_list.append(epoch_loss_dyn)\n",
    "                    val_loss_dynbg_list.append(epoch_loss_dyn_bg)\n",
    "                    val_loss_R_list.append(epoch_loss_R)\n",
    "                    val_loss_I_list.append(epoch_loss_I)\n",
    "\n",
    "                if phase == 'val' and epoch_loss_rec < best_loss_rec:\n",
    "                    best_loss_rec = epoch_loss_rec\n",
    "                    best_model_head_wts = copy.deepcopy(model.model_head.state_dict())\n",
    "                    best_model_sup_wts  = copy.deepcopy(model.model_sup.state_dict())\n",
    "                    best_model_dyn_wts  = copy.deepcopy(model.model_dyn.state_dict())\n",
    "                    \n",
    "            if epoch_loss_dyn_bg < parameters.relative_err:\n",
    "                break\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val reconstruction loss: {:4e}'.format(best_loss_rec))\n",
    "\n",
    "        save_dir_model_head = \"ckpts/\" + dir_name + \"/finetune\" + str(step*2 + status) + \"_head_epoch\"+ str(num_epochs)\n",
    "        save_dir_model_sup  = \"ckpts/\" + dir_name + \"/finetune\" + str(step*2 + status) + \"_sup_epoch\" + str(num_epochs)\n",
    "        save_dir_model_dyn  = \"ckpts/\" + dir_name + \"/finetune\" + str(step*2 + status) + \"_dyn_epoch\" + str(num_epochs)\n",
    "        print(\"saving model_head at \" + save_dir_model_head)\n",
    "        print(\"saving model_sup at \"  + save_dir_model_sup)\n",
    "        print(\"saving model_dyn at \"  + save_dir_model_dyn)\n",
    "        torch.save(best_model_head_wts, save_dir_model_head)\n",
    "        torch.save(best_model_sup_wts,  save_dir_model_sup)\n",
    "        torch.save(best_model_dyn_wts,  save_dir_model_dyn)\n",
    "\n",
    "        save_dir_loss  = \"train_loss/\" + dir_name + \"/finetune\" + str(step*2 + status) + \"_epoch\" + str(num_epochs)\n",
    "        print(\"saving loss at \" + save_dir_loss)\n",
    "        np.savez(save_dir_loss,\n",
    "                 train_loss_rec   = train_loss_rec_list,   val_loss_rec   = val_loss_rec_list, \n",
    "                 train_loss_dyn   = train_loss_dyn_list,   val_loss_dyn   = val_loss_dyn_list,\n",
    "                 train_loss_dynbg = train_loss_dynbg_list, val_loss_dynbg = val_loss_dynbg_list,\n",
    "                 train_loss_R     = train_loss_R_list,     val_loss_R     = val_loss_R_list, \n",
    "                 train_loss_I     = train_loss_I_list,     val_loss_I     = val_loss_I_list,\n",
    "                 time = time_elapsed)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
